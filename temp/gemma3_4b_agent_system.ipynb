{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa2a256",
   "metadata": {},
   "source": [
    "# í–¥ìƒëœ ì—­í•  íŠ¹í™” ë° ë‹¨ê³„ë³„ ì¶”ë¡  ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ (Gemma3-4B ì ìš©)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Google Colab TPU í™˜ê²½ì—ì„œ Gemma3-4B Instruction-tuned ëª¨ë¸ì„ ì‚¬ìš©í•œ ì—­í•  íŠ¹í™” ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ë³€ê²½/ê°œì„ \n",
    "- ëª¨ë¸ IDë¥¼ 2Bì—ì„œ 3-4B IT ë²„ì „ìœ¼ë¡œ ë³€ê²½\n",
    "- TPU í™˜ê²½ì—ì„œëŠ” Transformers ëŒ€ì‹  JAX/Flax ê¸°ë°˜ ë¡œë”© ì˜ˆì‹œ ì¶”ê°€\n",
    "- GPU í™˜ê²½ì—ì„œëŠ” BitsAndBytesConfigë¥¼ Q4_0ìœ¼ë¡œ ì„¤ì •\n",
    "- Accelerateë¥¼ í™œìš©í•œ TPU ì¥ì¹˜ ë§¤í•‘ ì˜ˆì‹œ ì¶”ê°€\n",
    "- ì—­í•  ë¼ìš°íŒ… ë¡œì§ ê°œì„  ì œì•ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install -q -U transformers accelerate bitsandbytes torch torch_xla jax jaxlib flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf671dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. TPU ì‚¬ìš© í™•ì¸ ë° ê³µí†µ ì„¤ì •\n",
    "import os, torch\n",
    "\n",
    "on_tpu = False\n",
    "if os.environ.get('COLAB_TPU_ADDR'):\n",
    "    on_tpu = True\n",
    "    print(\"âœ… TPU í™˜ê²½ ê°ì§€ë¨: JAX/Flax ë¡œë”© ì‚¬ìš© ê¶Œì¥ (bf16).\")\n",
    "else:\n",
    "    print(\"ğŸš« TPU ë¯¸ê°ì§€: GPU/CPU í™˜ê²½ìœ¼ë¡œ ë¡œë”©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5114c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3A. [TPU ì „ìš©] JAX/Flax ë¡œë”© ì˜ˆì‹œ (Gemma3-4B)\n",
    "if on_tpu:\n",
    "    os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "    os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n",
    "\n",
    "    from transformers import FlaxAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    model_id = \"google/gemma-3-4b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = FlaxAutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=\"bfloat16\",\n",
    "        _do_init=False\n",
    "    )\n",
    "    print(f\"{model_id} Flax ë¡œë”© ì™„ë£Œ (dtype=bfloat16).\")\n",
    "\n",
    "    import jax\n",
    "    @jax.pmap\n",
    "    def infer_fn(input_ids, attention_mask):\n",
    "        return model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "    print(\"âœ… JAX/Flax pmap inference ì¤€ë¹„ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd76d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3B. [GPU/CPU ê³µí†µ] Transformers + BitsAndBytes ë¡œë”© ì˜ˆì‹œ\n",
    "else:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "    from accelerate import init_empty_weights, infer_auto_device_map\n",
    "\n",
    "    model_id = \"google/gemma-3-4b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    use_quant = torch.cuda.is_available()\n",
    "    quant_cfg = None\n",
    "    if use_quant:\n",
    "        quant_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        print(\"ğŸ”§ GPU ì–‘ìí™” í™œì„±í™”: Q4_0 (nf4)\")\n",
    "\n",
    "    with init_empty_weights():\n",
    "        dummy = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quant_cfg)\n",
    "\n",
    "    device_map = infer_auto_device_map(dummy, no_split_module_classes=[\"GPTJBlock\"])\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quant_cfg,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=torch.bfloat16 if not use_quant else None\n",
    "    )\n",
    "    print(f\"{model_id} Transformers ë¡œë”© ì™„ë£Œ on {device_map}.\")\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    print(\"âœ… Transformers pipeline ìƒì„± ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6caf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. í–¥ìƒëœ ì—­í•  íŠ¹í™” ì—ì´ì „íŠ¸ ì •ì˜\n",
    "agent_roles = {\n",
    "    # ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ì •ì˜\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1dca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 5. ìš”ì²­ ë¼ìš°íŒ… ë¡œì§ ê°œì„ \n",
    "import re\n",
    "def route_request(user_query):\n",
    "    q = user_query.lower()\n",
    "    if re.search(r\"\\b(íŒŒì´ì¬ ì½”ë“œ|generate code)\\b\", q):\n",
    "        return \"code_generator\"\n",
    "    if re.search(r\"\\b(ìš”ì•½í•´ì¤˜|summarize)\\b\", q):\n",
    "        return \"summarizer\"\n",
    "    if re.search(r\"\\b(ì˜ì–´ë¡œ ë²ˆì—­)\\b\", q):\n",
    "        return \"translator_ko_en\"\n",
    "    return \"general_assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5600b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. ì¶”ë¡  ì‹¤í–‰ í•¨ìˆ˜\n",
    "def generate_response_agentic(pipeline_instance, role_id, user_query):\n",
    "    if on_tpu:\n",
    "        inputs = tokenizer(user_query, return_tensors=\"jax\", padding=True)\n",
    "        logits = infer_fn(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        return \"âš ï¸ TPU ìƒ˜í”Œë§ í•¨ìˆ˜ëŠ” ì§ì ‘ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    else:\n",
    "        return pipe(f\"{agent_roles[role_id]['system_prompt']}\\n{user_query}\")[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. ë°ëª¨ ì‹¤í–‰\n",
    "if not on_tpu:\n",
    "    for q in [\"ê°„ë‹¨í•œ íŒŒì´ì¬ í•¨ìˆ˜ ìƒì„±í•´ì¤˜\", \"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì¤˜: ...\"]:\n",
    "        role = route_request(q)\n",
    "        print(f\"\\nâ–¶ {role} ì²˜ë¦¬:\", generate_response_agentic(pipe, role, q))\n",
    "else:\n",
    "    print(\"â–¶ TPU í™˜ê²½: JAX inference ì½”ë“œ ì‘ì„± í›„ í…ŒìŠ¤íŠ¸ í•„ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7a126",
   "metadata": {},
   "source": [
    "## 8. ê²°ë¡  ë° í–¥í›„ ê³¼ì œ\n",
    "\n",
    "- Gemma3-4B ëª¨ë¸ë¡œ í™•ì¥í•˜ì—¬ ë” í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ ë° ì„±ëŠ¥ í™•ë³´\n",
    "- TPU í™˜ê²½ì—ì„œëŠ” Transformers ëŒ€ì‹  JAX/Flax ì‚¬ìš© ê¶Œì¥ (bf16)\n",
    "- GPU í™˜ê²½ì—ì„œëŠ” BitsAndBytesConfigë¥¼ Q4_0(nf4)ìœ¼ë¡œ ì„¤ì •í•´ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "- Accelerateì˜ init_empty_weights + infer_auto_device_mapìœ¼ë¡œ ëŒ€í˜• ëª¨ë¸ ìë™ ë¶„ì‚° ë°°ì¹˜\n",
    "- í–¥í›„: \n",
    "  - TPUìš© ìƒ˜í”Œë§/ë””ì½”ë”© ë¡œì§ êµ¬í˜„  \n",
    "  - LangChain/AI agent í”„ë ˆì„ì›Œí¬ì™€ ê²°í•©í•œ ì‹¤ì œ ì—ì´ì „íŠ¸ ì œì–´  \n",
    "  - NLU intent ë¶„ë¥˜ ëª¨ë¸ì„ í†µí•œ ì •êµí•œ ë¼ìš°íŒ…"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
