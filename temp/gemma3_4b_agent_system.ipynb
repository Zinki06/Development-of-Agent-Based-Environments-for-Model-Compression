{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa2a256",
   "metadata": {},
   "source": [
    "# 향상된 역할 특화 및 단계별 추론 에이전트 시스템 (Gemma3-4B 적용)\n",
    "\n",
    "이 노트북은 Google Colab TPU 환경에서 Gemma3-4B Instruction-tuned 모델을 사용한 역할 특화 에이전트 시스템을 구축합니다.\n",
    "\n",
    "## 변경/개선\n",
    "- 모델 ID를 2B에서 3-4B IT 버전으로 변경\n",
    "- TPU 환경에서는 Transformers 대신 JAX/Flax 기반 로딩 예시 추가\n",
    "- GPU 환경에서는 BitsAndBytesConfig를 Q4_0으로 설정\n",
    "- Accelerate를 활용한 TPU 장치 매핑 예시 추가\n",
    "- 역할 라우팅 로직 개선 제안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. 환경 설정 및 라이브러리 설치\n",
    "!pip install -q -U transformers accelerate bitsandbytes torch torch_xla jax jaxlib flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf671dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. TPU 사용 확인 및 공통 설정\n",
    "import os, torch\n",
    "\n",
    "on_tpu = False\n",
    "if os.environ.get('COLAB_TPU_ADDR'):\n",
    "    on_tpu = True\n",
    "    print(\"✅ TPU 환경 감지됨: JAX/Flax 로딩 사용 권장 (bf16).\")\n",
    "else:\n",
    "    print(\"🚫 TPU 미감지: GPU/CPU 환경으로 로딩.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5114c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3A. [TPU 전용] JAX/Flax 로딩 예시 (Gemma3-4B)\n",
    "if on_tpu:\n",
    "    os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "    os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n",
    "\n",
    "    from transformers import FlaxAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    model_id = \"google/gemma-3-4b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = FlaxAutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=\"bfloat16\",\n",
    "        _do_init=False\n",
    "    )\n",
    "    print(f\"{model_id} Flax 로딩 완료 (dtype=bfloat16).\")\n",
    "\n",
    "    import jax\n",
    "    @jax.pmap\n",
    "    def infer_fn(input_ids, attention_mask):\n",
    "        return model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "    print(\"✅ JAX/Flax pmap inference 준비 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd76d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3B. [GPU/CPU 공통] Transformers + BitsAndBytes 로딩 예시\n",
    "else:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "    from accelerate import init_empty_weights, infer_auto_device_map\n",
    "\n",
    "    model_id = \"google/gemma-3-4b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    use_quant = torch.cuda.is_available()\n",
    "    quant_cfg = None\n",
    "    if use_quant:\n",
    "        quant_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        print(\"🔧 GPU 양자화 활성화: Q4_0 (nf4)\")\n",
    "\n",
    "    with init_empty_weights():\n",
    "        dummy = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quant_cfg)\n",
    "\n",
    "    device_map = infer_auto_device_map(dummy, no_split_module_classes=[\"GPTJBlock\"])\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quant_cfg,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=torch.bfloat16 if not use_quant else None\n",
    "    )\n",
    "    print(f\"{model_id} Transformers 로딩 완료 on {device_map}.\")\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    print(\"✅ Transformers pipeline 생성 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6caf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. 향상된 역할 특화 에이전트 정의\n",
    "agent_roles = {\n",
    "    # 기존과 동일하게 정의\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1dca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 5. 요청 라우팅 로직 개선\n",
    "import re\n",
    "def route_request(user_query):\n",
    "    q = user_query.lower()\n",
    "    if re.search(r\"\\b(파이썬 코드|generate code)\\b\", q):\n",
    "        return \"code_generator\"\n",
    "    if re.search(r\"\\b(요약해줘|summarize)\\b\", q):\n",
    "        return \"summarizer\"\n",
    "    if re.search(r\"\\b(영어로 번역)\\b\", q):\n",
    "        return \"translator_ko_en\"\n",
    "    return \"general_assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5600b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. 추론 실행 함수\n",
    "def generate_response_agentic(pipeline_instance, role_id, user_query):\n",
    "    if on_tpu:\n",
    "        inputs = tokenizer(user_query, return_tensors=\"jax\", padding=True)\n",
    "        logits = infer_fn(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        return \"⚠️ TPU 샘플링 함수는 직접 구현해야 합니다.\"\n",
    "    else:\n",
    "        return pipe(f\"{agent_roles[role_id]['system_prompt']}\\n{user_query}\")[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. 데모 실행\n",
    "if not on_tpu:\n",
    "    for q in [\"간단한 파이썬 함수 생성해줘\", \"다음 텍스트를 요약해줘: ...\"]:\n",
    "        role = route_request(q)\n",
    "        print(f\"\\n▶ {role} 처리:\", generate_response_agentic(pipe, role, q))\n",
    "else:\n",
    "    print(\"▶ TPU 환경: JAX inference 코드 작성 후 테스트 필요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7a126",
   "metadata": {},
   "source": [
    "## 8. 결론 및 향후 과제\n",
    "\n",
    "- Gemma3-4B 모델로 확장하여 더 풍부한 컨텍스트 및 성능 확보\n",
    "- TPU 환경에서는 Transformers 대신 JAX/Flax 사용 권장 (bf16)\n",
    "- GPU 환경에서는 BitsAndBytesConfig를 Q4_0(nf4)으로 설정해 메모리 절약\n",
    "- Accelerate의 init_empty_weights + infer_auto_device_map으로 대형 모델 자동 분산 배치\n",
    "- 향후: \n",
    "  - TPU용 샘플링/디코딩 로직 구현  \n",
    "  - LangChain/AI agent 프레임워크와 결합한 실제 에이전트 제어  \n",
    "  - NLU intent 분류 모델을 통한 정교한 라우팅"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
